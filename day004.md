## MNIST ##
- MNIST数据集
	- 每一张图片都表示一个0-9之间的数字
- softmax回归步骤
	1. 对输入，被分类对象属于某个类的‘证据’相加求和
	2. 将这个‘证据’的和转化为概率

- 权重
	- 负数	明显不属于
	- 正数	非常有利于

### 编程 ###

	import tensorflow as tf
	import random
	import matplotlib.pyplot as plt
	
	tf.set_random_seed(777)  # for reproducibility
	
	from tensorflow.examples.tutorials.mnist import input_data
	# Check out https://www.tensorflow.org/get_started/mnist/beginners for
	# more information about the mnist dataset
	# 在当前目录下，创建MNIST_data文件夹，并自动下载训练集、验证集和测试集
	mnist = input_data.read_data_sets("MNIST_data/", one_hot=True) 
	
	nb_classes = 10
	
	# MNIST data image of shape 28 * 28 = 784
	X = tf.placeholder(tf.float32, [None, 784])
	# 0 - 9 digits recognition = 10 classes
	Y = tf.placeholder(tf.float32, [None, nb_classes])
	
	W = tf.Variable(tf.random_normal([784, nb_classes]))
	b = tf.Variable(tf.random_normal([nb_classes]))
	
	# Hypothesis (using softmax)
	hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)
	
	cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))
	optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)
	
	# Test model
	is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))
	# Calculate accuracy
	accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
	
	# parameters
	training_epochs = 15
	batch_size = 100
	
	with tf.Session() as sess:
	    # Initialize TensorFlow variables
	    sess.run(tf.global_variables_initializer())
	    # Training cycle
	    for epoch in range(training_epochs):
	        avg_cost = 0
	        total_batch = int(mnist.train.num_examples / batch_size)
	
	        for i in range(total_batch):
	            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
	            c, _ = sess.run([cost, optimizer], feed_dict={
	                X: batch_xs, Y: batch_ys})
	            avg_cost += c / total_batch
	
	        print('Epoch:', '%04d' % (epoch + 1),
	              'cost =', '{:.9f}'.format(avg_cost))
	
	    print("Learning finished")
	
	    # Test the model using test sets
	    print("Accuracy: ", accuracy.eval(session=sess, feed_dict={
	        X: mnist.test.images, Y: mnist.test.labels}))
	
	    # Get one and predict
	    r = random.randint(0, mnist.test.num_examples - 1)
	    print("Label: ", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))
	    print("Prediction: ", sess.run(
	        tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))
	
	    plt.imshow(
	        mnist.test.images[r:r + 1].reshape(28, 28),
	        cmap='Greys',
	        interpolation='nearest')
	    plt.show()

	“”“		
		Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
		Extracting MNIST_data/train-images-idx3-ubyte.gz
		Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
		Extracting MNIST_data/train-labels-idx1-ubyte.gz
		Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
		Extracting MNIST_data/t10k-images-idx3-ubyte.gz
		Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
		Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
		
		Epoch: 0001 cost = 2.826302754
		Epoch: 0002 cost = 1.061668976
		Epoch: 0003 cost = 0.838061331
		Epoch: 0004 cost = 0.733232755
		Epoch: 0005 cost = 0.669279896
		Epoch: 0006 cost = 0.624611842
		Epoch: 0007 cost = 0.591160356
		Epoch: 0008 cost = 0.563868994
		Epoch: 0009 cost = 0.541745182
		Epoch: 0010 cost = 0.522673586
		Epoch: 0011 cost = 0.506782330
		Epoch: 0012 cost = 0.492447647
		Epoch: 0013 cost = 0.479955835
		Epoch: 0014 cost = 0.468893671
		Epoch: 0015 cost = 0.458703482
		Learning finished
		Accuracy:  0.8951
		Label:  [7]
		Prediction:  [7]

	“”“
[参阅](https://github.com/hunkim/DeepLearningZeroToAll/blob/master/lab-07-4-mnist_introduction.py)

[训练集、验证集、测试集](http://yann.lecun.com/exdb/mnist/)