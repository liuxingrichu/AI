## Neural Net ##

###  softmax for mnist ###

cost改为交叉熵

	#!/usr/bin/env python
	# -*- coding:utf-8 -*-
	
	import tensorflow as tf
	import random
	import matplotlib.pyplot as plt
	from tensorflow.examples.tutorials.mnist import input_data
	
	tf.set_random_seed(777)  # reproducibility
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
	# Check out https://www.tensorflow.org/get_started/mnist/beginners for
	# more information about the mnist dataset
	
	# parameters
	learning_rate = 0.001
	training_epochs = 15
	batch_size = 100
	
	# input place holders
	X = tf.placeholder(tf.float32, [None, 784])
	Y = tf.placeholder(tf.float32, [None, 10])
	
	# weights & bias for nn layers
	W = tf.Variable(tf.random_normal([784, 10]))
	b = tf.Variable(tf.random_normal([10]))
	
	hypothesis = tf.matmul(X, W) + b
	
	# define cost/loss & optimizer
	cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
	    logits=hypothesis, labels=Y))
	optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)
	
	# initialize
	sess = tf.Session()
	sess.run(tf.global_variables_initializer())
	
	# train my model
	for epoch in range(training_epochs):
	    avg_cost = 0
	    total_batch = int(mnist.train.num_examples / batch_size)
	
	    for i in range(total_batch):
	        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
	        feed_dict = {X: batch_xs, Y: batch_ys}
	        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)
	        avg_cost += c / total_batch
	
	    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))
	
	print('Learning Finished!')
	
	# Test model and check accuracy
	correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))
	accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
	print('Accuracy:', sess.run(accuracy, feed_dict={
	    X: mnist.test.images, Y: mnist.test.labels}))
	
	# Get one and predict
	r = random.randint(0, mnist.test.num_examples - 1)
	print("Label: ", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))
	print("Prediction: ", sess.run(
	    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))
	
	plt.imshow(mnist.test.images[r:r + 1].
	           reshape(28, 28), cmap='Greys', interpolation='nearest')
	plt.show()

	
	"""
		Extracting MNIST_data/train-images-idx3-ubyte.gz
		Extracting MNIST_data/train-labels-idx1-ubyte.gz
		Extracting MNIST_data/t10k-images-idx3-ubyte.gz
		Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
		
		Epoch: 0001 cost = 5.745170969
		Epoch: 0002 cost = 1.780056698
		Epoch: 0003 cost = 1.122778631
		Epoch: 0004 cost = 0.872012247
		Epoch: 0005 cost = 0.738203178
		Epoch: 0006 cost = 0.654728883
		Epoch: 0007 cost = 0.596023602
		Epoch: 0008 cost = 0.552216816
		Epoch: 0009 cost = 0.518254962
		Epoch: 0010 cost = 0.491113193
		Epoch: 0011 cost = 0.468347534
		Epoch: 0012 cost = 0.449374341
		Epoch: 0013 cost = 0.432675650
		Epoch: 0014 cost = 0.418828148
		Epoch: 0015 cost = 0.406128927
		Learning Finished!
		Accuracy: 0.9023 准确度
		Label:  [3]
		Prediction:  [3]

	"""
[参阅mnist_softmax](https://github.com/hunkim/DeepLearningZeroToAll/blob/master/lab-10-1-mnist_softmax.py)


### NN for MNIST ###

	#!/usr/bin/env python
	# -*- coding:utf-8 -*-
	
	import tensorflow as tf
	import random
	import matplotlib.pyplot as plt
	
	from tensorflow.examples.tutorials.mnist import input_data
	
	tf.set_random_seed(777)  # reproducibility
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
	# Check out https://www.tensorflow.org/get_started/mnist/beginners for
	# more information about the mnist dataset
	
	# parameters
	learning_rate = 0.001
	training_epochs = 15
	batch_size = 100
	
	# input place holders
	X = tf.placeholder(tf.float32, [None, 784])
	Y = tf.placeholder(tf.float32, [None, 10])
	
	# weights & bias for nn layers
	W1 = tf.Variable(tf.random_normal([784, 256]))
	b1 = tf.Variable(tf.random_normal([256]))
	L1 = tf.nn.relu(tf.matmul(X, W1) + b1)
	
	W2 = tf.Variable(tf.random_normal([256, 256]))
	b2 = tf.Variable(tf.random_normal([256]))
	L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)
	
	W3 = tf.Variable(tf.random_normal([256, 10]))
	b3 = tf.Variable(tf.random_normal([10]))
	hypothesis = tf.matmul(L2, W3) + b3
	
	# define cost/loss & optimizer
	cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
	    logits=hypothesis, labels=Y))
	optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)
	
	# initialize
	sess = tf.Session()
	sess.run(tf.global_variables_initializer())
	
	# train my model
	for epoch in range(training_epochs):
	    avg_cost = 0
	    total_batch = int(mnist.train.num_examples / batch_size)
	
	    for i in range(total_batch):
	        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
	        feed_dict = {X: batch_xs, Y: batch_ys}
	        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)
	        avg_cost += c / total_batch
	
	    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))
	
	print('Learning Finished!')
	
	# Test model and check accuracy
	correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))
	accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
	print('Accuracy:', sess.run(accuracy, feed_dict={
	    X: mnist.test.images, Y: mnist.test.labels}))
	
	# Get one and predict
	r = random.randint(0, mnist.test.num_examples - 1)
	print("Label: ", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))
	print("Prediction: ", sess.run(
	    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))
	
	plt.imshow(mnist.test.images[r:r + 1].
	           reshape(28, 28), cmap='Greys', interpolation='nearest')
	plt.show()

	"""
		Extracting MNIST_data/train-images-idx3-ubyte.gz
		Extracting MNIST_data/train-labels-idx1-ubyte.gz
		Extracting MNIST_data/t10k-images-idx3-ubyte.gz
		Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
		
		Epoch: 0001 cost = 142.435877703
		Epoch: 0002 cost = 38.927020280
		Epoch: 0003 cost = 24.386520609
		Epoch: 0004 cost = 16.854838913
		Epoch: 0005 cost = 12.198336699
		Epoch: 0006 cost = 9.148108485
		Epoch: 0007 cost = 6.862091394
		Epoch: 0008 cost = 5.080390849
		Epoch: 0009 cost = 3.751601476
		Epoch: 0010 cost = 2.896519461
		Epoch: 0011 cost = 2.202559267
		Epoch: 0012 cost = 1.656186669
		Epoch: 0013 cost = 1.264444393
		Epoch: 0014 cost = 0.983772296
		Epoch: 0015 cost = 0.794967118
		Learning Finished!
		Accuracy: 0.9433	准确度
		Label:  [9]
		Prediction:  [9]

	"""
[参阅mnist_nn](https://github.com/hunkim/DeepLearningZeroToAll/blob/master/lab-10-2-mnist_nn.py)