## TensorFlow ##

### 梯度 ###
梯度是一个向量；每个元素为函数对一元变量的偏导数；
它既有大小（其大小为最大方向导数），也有方向。

### 导数 ###
导数是函数的局部性质。

一个函数在某一点的导数描述了这个函数在这一点附近的变化率。

如果函数的自变量和取值都是实数的话，函数在某一点的导数就是该函数所代表的曲线在这一点上的切线斜率。

导数的本质是通过极限的概念对函数进行局部的线性逼近。

例如在运动学中，物体的位移对于时间的导数就是物体的瞬时速度。

### Logistic regression（Binary classification） ###

	#!/usr/bin/env python
	# -*- coding:utf-8 -*-
	
	import tensorflow as tf
	
	tf.set_random_seed(777)  # for reproducibility
	
	x_data = [[1, 2],
	          [2, 3],
	          [3, 1],
	          [4, 3],
	          [5, 3],
	          [6, 2]]
	y_data = [[0],
	          [0],
	          [0],
	          [1],
	          [1],
	          [1]]
	
	# placeholders for a tensor that will be always fed.
	X = tf.placeholder(tf.float32, shape=[None, 2])
	Y = tf.placeholder(tf.float32, shape=[None, 1])
	
	W = tf.Variable(tf.random_normal([2, 1]), name='weight')
	b = tf.Variable(tf.random_normal([1]), name='bias')
	
	# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))
	hypothesis = tf.sigmoid(tf.matmul(X, W) + b)
	
	# cost/loss function
	cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *
	                       tf.log(1 - hypothesis))
	
	train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)
	
	# Accuracy computation
	# True if hypothesis>0.5 else False
	predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
	accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))
	
	# Launch graph
	with tf.Session() as sess:
	    # Initialize TensorFlow variables
	    sess.run(tf.global_variables_initializer())
	
	    for step in range(10001):
	        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})
	        if step % 200 == 0:
	            print(step, cost_val)
	
	    # Accuracy report
	    h, c, a = sess.run([hypothesis, predicted, accuracy],
	                       feed_dict={X: x_data, Y: y_data})
	print("\nHypothesis: ", h, "\nCorrect (Y): ", c, "\nAccuracy: ", a)

	"""
	
		0 1.7307833
		200 0.5715119
		...
		2200 0.34882212
		2400 0.3382263
		...
		9000 0.16165252
		9200 0.15906553
		9400 0.1565599
		9600 0.15413196
		9800 0.15177831
		10000 0.14949562
		
		Hypothesis:  [[0.03074029]
		 [0.15884677]
		 [0.30486736]
		 [0.78138196]
		 [0.93957496]
		 [0.9801688 ]] 
		Correct (Y):  [[0.]
		 [0.]
		 [0.]
		 [1.]
		 [1.]
		 [1.]] 
		Accuracy:  1.0

	"""

[参阅](https://github.com/hunkim/DeepLearningZeroToAll/blob/master/lab-05-1-logistic_regression.py)


	#!/usr/bin/env python
	# -*- coding:utf-8 -*-
	
	import tensorflow as tf
	import numpy as np
	tf.set_random_seed(777)  # for reproducibility
	
	xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)
	x_data = xy[:, 0:-1]
	y_data = xy[:, [-1]]
	
	print(x_data.shape, y_data.shape)
	
	# placeholders for a tensor that will be always fed.
	X = tf.placeholder(tf.float32, shape=[None, 8])
	Y = tf.placeholder(tf.float32, shape=[None, 1])
	
	W = tf.Variable(tf.random_normal([8, 1]), name='weight')
	b = tf.Variable(tf.random_normal([1]), name='bias')
	
	# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))
	hypothesis = tf.sigmoid(tf.matmul(X, W) + b)
	
	# cost/loss function
	cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *
	                       tf.log(1 - hypothesis))
	
	train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)
	
	# Accuracy computation
	# True if hypothesis>0.5 else False
	predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
	accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))
	
	# Launch graph
	with tf.Session() as sess:
	    # Initialize TensorFlow variables
	    sess.run(tf.global_variables_initializer())
	
	    for step in range(10001):
	        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})
	        if step % 200 == 0:
	            print(step, cost_val)
	
	    # Accuracy report
	    h, c, a = sess.run([hypothesis, predicted, accuracy],
	                       feed_dict={X: x_data, Y: y_data})
	print("\nHypothesis: ", h, "\nCorrect (Y): ", c, "\nAccuracy: ", a)
		
	"""
		(759, 8) (759, 1)
	
		0 0.82793975
		200 0.7551808
		...
		9600 0.4920562
		9800 0.49139574
		10000 0.49076667
	
	
		 [0.]
		 [1.]
		 [1.]
		 [1.]
		 [1.]
		 [1.]
		 [1.]] 
		Accuracy:  0.7628459

	"""
[参阅](https://github.com/hunkim/DeepLearningZeroToAll/blob/master/lab-05-2-logistic_regression_diabetes.py)