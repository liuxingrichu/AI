## tensorflow ##
### 版本 ###
	import tensorflow as tf
	tf.__version__

### tensorflow mechanics ###
#### 方式一 ####
1. build graph using tensorflow operations
2. feed data and run graph(operation) sess.run(op)
3. update variables in the graph (and return values)

#### 方式二 ####
1. build graph using tensorflow operations
2. feed data and run graph(operation) sess.run(op， feed_dict={x:x_data})
3. update variables in the graph (and return values)

### How to minimize cost ###
1. start with initial guesses
	1. start at 0,0 or any other value
	2. keeping changing W and b a litte bit to try and reduce cost(W, b)
2. each time you change the parameters, you select the gradient which reduces cost(W, b) the most possible
3. repeat
4. do so until you converge to a local minimum
5. has an interesting property
	1. where you start can determine which minimum you end up


### 线性回归 Linear Regression ###
研究对象： H(x) = Wx + b

判定依据(方差)： cost(W) = 1/m * sum[(H(X)-y)^2]

梯度： W := W - a * [对cost(W)求导]

目标： minimize(cost(W, b))

方差：度量随机变量和其数学期望（即均值）之间的偏离程度

- 均值（平均数）
	- 对分布中心所在位置的度量
	- 所有观测值的和，除以观测值个数
	- 极端值对均值影响极大

#### 实现：方式一 ####

	#!/usr/bin/env python
	# -*- coding:utf-8 -*-
	
	import tensorflow as tf
	
	# X and Y data
	x_train = [1, 2, 3]
	y_train = [1, 2, 3]
	
	W = tf.Variable(tf.random_normal([1]), name='weight')
	b = tf.Variable(tf.random_normal([1]), name='bias')
	
	# our hypothesis XW + b
	hypothesis = x_train * W + b
	# cost/loss function: 方差
	cost = tf.reduce_mean(tf.square(hypothesis - y_train))
	# minimize：求导
	optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
	train = optimizer.minimize(cost)
	# launch the graph in a session
	sess = tf.Session()
	# initializes global variables in the graph
	sess.run(tf.global_variables_initializer())
	# fit the line
	for step in range(2001):
	    sess.run(train)
	    if step % 20 == 0:
	        print(step, sess.run(cost), sess.run(W), sess.run(b))

#### 实现：方式二 ####

	#!/usr/bin/env python
	# -*- coding:utf-8 -*-
	
	import tensorflow as tf
	
	# X and Y data
	x_train = tf.placeholder(tf.float32, shape=[None])
	y_train = tf.placeholder(tf.float32, shape=[None])
	
	W = tf.Variable(tf.random_normal([1]), name='weight')
	b = tf.Variable(tf.random_normal([1]), name='bias')
	
	# our hypothesis XW + b
	hypothesis = x_train * W + b
	# cost/loss function
	cost = tf.reduce_mean(tf.square(hypothesis - y_train))
	# minimize
	optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
	train = optimizer.minimize(cost)
	# launch the graph in a session
	sess = tf.Session()
	# initializes global variables in the graph
	sess.run(tf.global_variables_initializer())
	# fit the line
	for step in range(2001):
	    cost_val, W_val, b_val, _ = sess.run([cost, W, b, train],
	                                         feed_dict={x_train: [1, 2, 3],
	                                                    y_train: [1, 2, 3]})
	    if step % 20 == 0:
	        print(step, cost_val, W_val, b_val)

[在线导数计算器](https://www.derivative-calculator.net/)